{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New method of using RAG to enahnce LLM knowledge for picking book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from langchain_community.llms import Ollama # newer version of this import from langchain_community.chat_models import ChatOllama idk if it changes anything though\n",
    "from langchain_core.prompts import ChatPromptTemplate # Prompt templates convert raw user input to better input to the LLM (guide reponse)\n",
    "from langchain_core.output_parsers import StrOutputParser # original model output is a message but this function parses it to a string (easier to work with)\n",
    "from langchain_core.messages import HumanMessage, AIMessage # Used to frame history to LLM and retriever\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"goodreads_filtered.csv\"\n",
    "\n",
    "loader = CSVLoader(file_path=file_path,encoding='utf-8')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(data) # Idk the difference between texts and data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts1k = texts[0:1000]\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(  # 1k items took 38 mins, so 10 takes 6.3 hours\n",
    "    model=\"llama3\",\n",
    ")\n",
    "db = FAISS.from_documents(texts, ollama_emb)\n",
    "\n",
    "# Saving FAISS vector db locally\n",
    "#db.save_local(r\"Vector DB Storage\\FAISS_GoodreadsData1k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving FAISS vector db locally\n",
    "db.save_local(r\"Vector DB Storage\\FAISS_GoodreadsData10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the FAISS vector db from storage\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=\"llama3\",\n",
    ")\n",
    "db = FAISS.load_local(r\"Vector DB Storage\\FAISS_GoodreadsData10k\", ollama_emb, allow_dangerous_deserialization=True)\n",
    "\n",
    "print(db.index.ntotal) # works with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = db.similarity_search(\"horror and fantasy books for older patron\", n=5)\n",
    "for detail in answer:\n",
    "    print(detail.page_content.split('\\n')[0])\n",
    "    print(detail.page_content.split('\\n')[3])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of using an LLM to generate a list of book recs, I have have an LLM parse the user input extract and generate search terms that would be helpful with a book seach\n",
    "### Them use those terms provided by the LLM into a pattern finding/search python function to generate a list of books that way (Broader context then we have all the info right there without having to do a check), kinda like how perplexity works where the LLM refines the search inquiry to optimize it  \n",
    "### extract and generate key words from a query (LLM), database search using those generated key words to find books (python function), give book title and description to LLM to explain why these books are good in relation to original prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama # newer version of this import from langchain_community.chat_models import ChatOllama idk if it changes anything though\n",
    "from langchain_core.prompts import ChatPromptTemplate # Prompt templates convert raw user input to better input to the LLM (guide reponse)\n",
    "from langchain_core.output_parsers import StrOutputParser # original model output is a message but this function parses it to a string (easier to work with)\n",
    "from langchain_core.messages import HumanMessage, AIMessage # Used to frame history to LLM and retriever\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "from filelock import FileLock, Timeout\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM that provides a list of book reccomendations (title only) in the format of a python list\n",
    "\n",
    "model = \"llama3\" # mistral, llama2, kdl_copilot_llama3, llama3, llama2:13b\n",
    "llm = Ollama(model=model, format='json') # how to structure LLM output: https://python.langchain.com/v0.2/docs/how_to/structured_output/\n",
    "\n",
    "system = \"\"\"Your are a helpful librarian AI assistant. Given a user query about reccomendations for books to read you parse that sentence \\\n",
    "    returning and gerating key words to be used in a pattern seach for relavent books in a book database. \\\n",
    "    The key words can be a combinations of words the user query as well as relavent words you think will be helpful search terms find relavent books for them. \\\n",
    "    Output your response in json. \\\n",
    "    Include at least 8 relevant key words. \\ \n",
    "\n",
    "Here are some examples of proper json output based on an example prompts:\n",
    "\n",
    "example_user: I like mystery book, what are some recommendations like that if I just read Louise Penny?\n",
    "example_assistant: {{\"keyword_list\": [\n",
    "    \"Psychological depth\",\n",
    "    \"Mystery\",\n",
    "    \"Crime\",\n",
    "    \"Suspense\",\n",
    "    \"Character-driven\"\n",
    "    ]}}\n",
    "\n",
    "example_user: What books should I read after 1984 by George Orwell??\n",
    "example_assistant: {{\"keyword_list\": [\n",
    "    \"Dystopia\",\n",
    "    \"Totalitarianism\",\n",
    "    \"Big Brother\",\n",
    "    \"Surveillance\",\n",
    "    \"Political satire\",\n",
    "    \"Orwellian\"]}}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([  # LLM unreliable for ISBN number but does good for book titles\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "orig_prompt = \"I liked the mistborn series by sanderson, what are some books like it?\"\n",
    "\n",
    "output1 = chain.invoke({\"input\": orig_prompt}) \n",
    "\n",
    "print(output1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#LLm outputs info as a string so we need to load it as json to be able index into it \n",
    "llm_output = json.loads(output1)\n",
    "\n",
    "listers = llm_output[\"keyword_list\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pattern matching exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern matching search which I will not do \n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from CSV file\n",
    "df = pd.read_excel(r'Complete_Monster_List.xlsx')\n",
    "\n",
    "# List of search terms\n",
    "search_terms = ['Science fiction', 'Juvenile', 'Fantasy fiction', 'Magic']\n",
    "\n",
    "# Function to perform pattern search\n",
    "def search_pattern(row, terms):\n",
    "    genre = row['Specific Genre'] if pd.notna(row['Specific Genre']) else ''\n",
    "    audience = row['Audiance'] if pd.notna(row['Audiance']) else ''\n",
    "    subject = row['Subject'] if pd.notna(row['Subject']) else ''\n",
    "    return any(term in genre or term in audience or term in subject for term in terms)\n",
    "\n",
    "# Apply function to DataFrame\n",
    "df['match'] = df.apply(search_pattern, terms=listers, axis=1)\n",
    "\n",
    "# Filter DataFrame based on matches\n",
    "matched_df = df[df['match']]\n",
    "\n",
    "# Display the result\n",
    "print(matched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[books['Author'].str.contains(\"Rebecca\", case=False, na=False)] # na is used to ignore missing values\n",
    "\n",
    "\n",
    "filtered_books = books[books['Author'].str.contains(\"Rebecca\", case=False, na=False)]\n",
    "selected_columns = filtered_books[['Title', 'Author', 'Item Type', 'Rating']] # selecting only specific columns to be displayed in the output df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Title', 'Author', 'Item Type', 'Rating']:\n",
    "    # Check if any keyword matches the value in the current column using str.contains()\n",
    "    for keyword in keywords:\n",
    "        if books[books[column].str.contains(\"Rebecca\", case=False, na=False)]:\n",
    "            # If a match is found, add 1 to the ranking score\n",
    "            ranking_score += 1\n",
    "\n",
    "ranking_scores.append(ranking_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in books.iterrows():\n",
    "    print(row['Title'], row['Author'], row['Subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(dataframe, keywords):\n",
    "    # Initialize an empty list to store the ranking scores\n",
    "    ranking_scores = []\n",
    "\n",
    "    # Iterate through each index and row in the DataFrame\n",
    "    for index, row in enumerate(dataframe):\n",
    "        # Initialize ranking score for the current row\n",
    "        ranking_score = 0\n",
    "        \n",
    "        # Iterate through each column of interest\n",
    "        for column in ['Title', 'Author', 'Item Type', 'Rating']:\n",
    "            # Check if any keyword matches the value in the current column using str.contains()\n",
    "            for keyword in keywords:\n",
    "                if row[column].str.contains(keyword, case=False, na=False).any():\n",
    "                    # If a match is found, increment the ranking score\n",
    "                    ranking_score += 1\n",
    "        \n",
    "        # Append the ranking score for the current row to the list\n",
    "        ranking_scores.append(ranking_score)\n",
    "    \n",
    "    # Add ranking_scores list as a new column to the DataFrame\n",
    "    dataframe['Ranking Score'] = ranking_scores\n",
    "    \n",
    "    # Sort the DataFrame based on the ranking score in descending order\n",
    "    sorted_df = dataframe.sort_values(by='Ranking Score', ascending=False)\n",
    "    \n",
    "    # Reset index for better presentation\n",
    "    sorted_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Drop the ranking score column before returning the DataFrame\n",
    "    sorted_df.drop(columns=['Ranking Score'], inplace=True)\n",
    "    \n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame called 'books'\n",
    "\n",
    "# List of keywords to search for\n",
    "keywords = ['fantasy', 'adventure']\n",
    "\n",
    "# Perform keyword search\n",
    "search_result = keyword_search(books, keywords)\n",
    "\n",
    "# Display the search result\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd method! Normal rec into LLM explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def create_user_item_matrix(data):\n",
    "    return pd.DataFrame(data).T\n",
    "\n",
    "def print_matrix(user_item_matrix):\n",
    "    print(\"\\nUser-Item Rating Matrix:\")\n",
    "    print(\"------------------------\")\n",
    "    \n",
    "    max_user_length = max(len(user) for user in user_item_matrix.index)\n",
    "    max_book_length = max(len(book) for book in user_item_matrix.columns)\n",
    "    \n",
    "    print(f\"{' ':<{max_user_length+2}}\", end=\"\")\n",
    "    for book in user_item_matrix.columns:\n",
    "        print(f\"{book:<{max_book_length+2}}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for user in user_item_matrix.index:\n",
    "        print(f\"{user:<{max_user_length+2}}\", end=\"\")\n",
    "        for rating in user_item_matrix.loc[user]:\n",
    "            if rating == 0:\n",
    "                print(f\"{'â€”':<{max_book_length+2}}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"{rating:<{max_book_length+2}}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    total_cells = user_item_matrix.size\n",
    "    filled_cells = (user_item_matrix != 0).sum().sum()\n",
    "    sparsity = 1 - (filled_cells / total_cells)\n",
    "    print(f\"\\nMatrix Sparsity: {sparsity:.2%}\")\n",
    "    print(f\"Filled Cells: {filled_cells}/{total_cells}\")\n",
    "\n",
    "def get_recommendations(user_item_matrix, user_id, n=5):\n",
    "    user_similarity = cosine_similarity(user_item_matrix)\n",
    "    similar_users = user_similarity[user_id].argsort()[::-1][1:]\n",
    "    user_ratings = user_item_matrix.iloc[user_id]\n",
    "    unrated_books = user_ratings[user_ratings == 0].index\n",
    "    \n",
    "    recommendations = {}\n",
    "    for book in unrated_books:\n",
    "        pred_rating = 0\n",
    "        total_similarity = 0\n",
    "        for similar_user in similar_users:\n",
    "            if user_item_matrix.iloc[similar_user][book] != 0:\n",
    "                similarity = user_similarity[user_id][similar_user]\n",
    "                rating = user_item_matrix.iloc[similar_user][book]\n",
    "                pred_rating += similarity * rating\n",
    "                total_similarity += similarity\n",
    "        if total_similarity > 0:\n",
    "            recommendations[book] = pred_rating / total_similarity\n",
    "    \n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_recommendations[:n]\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'User1': {'Book1': 4, 'Book2': 3, 'Book3': 5, 'Book4': 0, 'Book5': 0},\n",
    "    'User2': {'Book1': 5, 'Book2': 0, 'Book3': 4, 'Book4': 2, 'Book5': 1},\n",
    "    'User3': {'Book1': 3, 'Book2': 4, 'Book3': 0, 'Book4': 5, 'Book5': 2},\n",
    "    'User4': {'Book1': 0, 'Book2': 5, 'Book3': 3, 'Book4': 4, 'Book5': 0},\n",
    "    'User5': {'Book1': 2, 'Book2': 0, 'Book3': 0, 'Book4': 3, 'Book5': 5}\n",
    "}\n",
    "\n",
    "# Create the user-item matrix\n",
    "user_item_matrix = create_user_item_matrix(data)\n",
    "\n",
    "# Print the matrix\n",
    "print_matrix(user_item_matrix)\n",
    "\n",
    "# Get recommendations for User1\n",
    "user_id = 4 # User1's index\n",
    "recommendations = get_recommendations(user_item_matrix, user_id)\n",
    "\n",
    "print(\"\\nRecommendations for User1:\")\n",
    "for book, rating in recommendations:\n",
    "    print(f\"{book}: Predicted rating = {rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW NEW idea! \n",
    "\n",
    "#### Bibliocommons has book read alikes (from novelist) for every book in our collection based on URL format: \"https://kdl.bibliocommons.com/v2/record/S174C605581/recommendations\" only adding 'recommendations' after each item. \n",
    "#### If we could gather a dict of all kdl items and their associated bibliocommons id, we could scrape those read a likes for an LLM to synthesize "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
